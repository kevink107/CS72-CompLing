{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9TqNz3HLbr2e"},"source":["# Examples for Homework 6.1: BERT masked words in English and Mandarin Chinese\n","Dartmouth College, LING48, Spring 2023<br>\n","Rolando Coto-Solano (Rolando.A.Coto.Solano@dartmouth.edu)\n","\n","I did not write this code. This code is from Stack Overflow:<br>\n","https://stackoverflow.com/questions/54978443/predicting-missing-words-in-a-sentence-natural-language-processing-model\n","\n","This file uses a pre-trained BERT to predict the word missing from a sentence. In this file, we are using an example in English. Study the code, and then carry out the following tasks:\n","\n","(1)\tStudy the links below, so you can gain a better understanding of how BERT works:\n","\n","http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/<br>\n","http://jalammar.github.io/illustrated-bert/\n","\n","(2)\tChange the model from an English-language BERT to the Mandarin Chinese pretrained BERT. You can find the names of the models here:<br>https://huggingface.co/transformers/pretrained_models.html\n","\n","(3)\tChange the input so that it predicts a missing Chinese character. For example, for the sentence 你是中 __ 人吗  *nǐ shì zhōng ___ rén ma* , it should predict that the missing character is 国 guó (which would form the sentence “are you Chinese?”). \n","\n","(4)\tSubmit a PDF/LibreOffice/Word document with a brief explanation of the lines that you changed in the code. Also, please submit the Python code with your modification, and a screenshot of your results.\n"]},{"cell_type":"code","metadata":{"id":"6EIsh1Svwx5v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684004039655,"user_tz":240,"elapsed":19069,"user":{"displayName":"Kevin King","userId":"18133619474798900682"}},"outputId":"1ab0aedc-abef-4f4d-f115-3c90441e5041"},"source":["!pip install pytorch-pretrained-bert"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.0.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.22.4)\n","Collecting boto3 (from pytorch-pretrained-bert)\n","  Downloading boto3-1.26.133-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.65.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch-pretrained-bert) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch-pretrained-bert) (16.0.3)\n","Collecting botocore<1.30.0,>=1.29.133 (from boto3->pytorch-pretrained-bert)\n","  Downloading botocore-1.29.133-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->pytorch-pretrained-bert)\n","  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.133->boto3->pytorch-pretrained-bert) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.133->boto3->pytorch-pretrained-bert) (1.16.0)\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.26.133 botocore-1.29.133 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.1\n"]}]},{"cell_type":"code","metadata":{"id":"D4hOERZ-wQMA","executionInfo":{"status":"ok","timestamp":1684004048684,"user_tz":240,"elapsed":9037,"user":{"displayName":"Kevin King","userId":"18133619474798900682"}}},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMk75Yhyw2i1","executionInfo":{"status":"ok","timestamp":1684004048686,"user_tz":240,"elapsed":8,"user":{"displayName":"Kevin King","userId":"18133619474798900682"}}},"source":["# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"qk_TOvffw5PE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684004099365,"user_tz":240,"elapsed":21752,"user":{"displayName":"Kevin King","userId":"18133619474798900682"}},"outputId":"6e9d67b2-2955-4e19-8268-eb47c9fd41e9"},"source":["# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n","\n","text = '你是中 [MASK] 人吗'\n","tokenized_text = tokenizer.tokenize(text)\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","masked_index = tokenized_text.index('[MASK]')\n","\n","# Create the segments tensors.\n","segments_ids = [0] * len(tokenized_text)\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n","\n","# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n","model.eval()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 382072689/382072689 [00:08<00:00, 42634359.53B/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): BertLayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): BertLayerNorm()\n","      )\n","      (decoder): Linear(in_features=768, out_features=21128, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"rNED-D72xEcE","executionInfo":{"status":"ok","timestamp":1684004099519,"user_tz":240,"elapsed":158,"user":{"displayName":"Kevin King","userId":"18133619474798900682"}}},"source":["# Predict all tokens\n","with torch.no_grad():\n","    predictions = model(tokens_tensor, segments_tensors)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTA9IrBDxILc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684004116135,"user_tz":240,"elapsed":152,"user":{"displayName":"Kevin King","userId":"18133619474798900682"}},"outputId":"c65ee93a-fc85-4078-efff-7310e07dacf4"},"source":["predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","\n","print(predicted_token)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["国\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UerkuakadhZx"},"execution_count":null,"outputs":[]}]}