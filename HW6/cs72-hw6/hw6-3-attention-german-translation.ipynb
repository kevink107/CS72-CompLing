{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"name":"NLP from Scratch: Annotated Attention","colab":{"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"W4qtx9nggECt"},"source":["# Example 6.3: Attention in Machine Translation\n","Dartmouth College, LING48, Spring 2023<br>\n","Rolando Coto-Solano (Rolando.A.Coto.Solano@dartmouth.edu)\n","\n","Please read the following posts so you can study more about attention in transformers:\n","\n","(1)\thttp://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/<br>\n","(2)\thttp://jalammar.github.io/illustrated-bert/<br>\n","(3)\thttp://jalammar.github.io/illustrated-gpt2/\n","\n","You need to perform four tasks:\n","\n","(1) Run the code as is. This will train the system for only one epoch.\n","\n","(2)\tChange the number of epochs from 1 to 4. Then, execute the training and evaluation code again. At the very end, the code will draw an attention plot that shows the attention matrix between the German sentence and the resulting English sentence. Copy-paste this plot onto a PDF/LibreOffice/Word document.\n","\n","(3)\tExplain the plot. Are there English words that are being generated by paying attention to more than one German word? \n","\n","(4)\tThe German sentence has an element labelled \\<unk\\>. What is it? What does it mean for the input sentence to have this?\n","\n","(4)\tReplace the German sentence _ein boston terrier läuft über saftig-grünes gras vor einem weißen zaun_ with any other sentence in German and then plot the attention matrix for that sentence. How is it behaving? (Remember to include a screen capture in your PDF/LibreOffice/Word document).\n"]},{"cell_type":"markdown","metadata":{"id":"fpCCguo-1m_l"},"source":["This post is the first in a series of articles about [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), a subfield of machine learning concerning the \n","interaction between computers and human language. This article will be focused on *attention*, a mechanism that forms the backbone of many state-of-the art language models, including Google's BERT ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)), and OpenAI's GPT-2 ([Radford et al., 2019](https://openai.com/blog/better-language-models/)). "]},{"cell_type":"markdown","metadata":{"id":"kPWt1COV1m_u"},"source":["First introduced in the paper *Neural Machine Translation by Jointly Learning to Align and Translate* ([Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)), attention was an improvement to the classic sequence-to-sequence model for language translation. Instead of trying to decode a translated sentence from single fixed-length vector encoding of the source sentence, the attention mechanism aims to create an alignment between the source words and target words. This alignment allows the model to take all of the source words into account when predicting each target word."]},{"cell_type":"markdown","metadata":{"id":"U4BFJTCu1m_u"},"source":["The next two sections of this post will consist of portions of the paper *Effective Approaches to Attention-based Neural Machine Translation* ([Luong et al., 2015](https://arxiv.org/abs/1508.04025)) along with my own comments and code in blockquotes.\n","\n","> Before continuing, it is important to have an understanding of LSTM networks. [Here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a great resource. The format of this post was inspired by [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)."]},{"cell_type":"code","metadata":{"id":"cih423At1m_u"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from tqdm.notebook import tqdm\n","import numpy as np\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3twCkWS1m_v"},"source":["## Attention-based Models\n","\n","Our various attention-based models are classified\n","into two broad categories, global and local. These\n","classes differ in terms of whether the \"attention\"\n","is placed on all source positions or on only a few\n","source positions. We illustrate these two model\n","types in Figure 2 and 3 respectively.\n","    \n","Common to these two types of models is the fact\n","that at each time step $t$ in the decoding phase, both\n","approaches first take as input the hidden state $h_t$\n","at the top layer of a stacking LSTM. The goal is\n","then to derive a context vector $c_t$\n","that captures relevant source-side information to help predict the\n","current target word $y_t$. While these models differ\n","in how the context vector $c_t$\n","is derived, they share\n","the same subsequent steps.\n","Specifically, given the target hidden state $h_t$ and\n","the source-side context vector $c_t$, we employ a\n","simple concatenation layer to combine the information from both vectors:\n","\n","$$ \\tilde{h_t} = \\tanh(W_c[c_t;h_t])     $$\n","\n","The attentional vector $\\tilde{h_t}$\n","is then fed through the\n","softmax layer to produce the predictive distribution formulated as:\n","\n","$$p(y_t | y_{\\lt t}, x) = \\text{softmax}(W_s\\tilde{h_t}) $$"]},{"cell_type":"markdown","metadata":{"id":"_CZDRwCp1m_v"},"source":["The attentional vector $\\tilde{h_t}$\n","is then fed through the\n","softmax layer to produce the predictive distribution formulated as:"]},{"cell_type":"markdown","metadata":{"id":"9qMlv9y-1m_v"},"source":["> With this information we can implement the decoder portion of our model, leaving the context vector $c_t$ to be calculated by our (not yet written) `Attention` module."]},{"cell_type":"code","metadata":{"id":"Ly2cx-AD1m_w"},"source":["class Decoder(nn.Module):\n","    \"\"\"\n","    Decode output from hidden state and context\n","    \"\"\"\n","    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout):\n","        super(Decoder, self).__init__()\n","        self.output_dim = output_dim\n","        self.embed = nn.Embedding(output_dim, embed_dim)\n","        # stacking LSTM\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.attention = Attention(hidden_dim) # we'll get to later\n","        self.wc = nn.Linear(hidden_dim * 2, hidden_dim)\n","        self.ws = nn.Linear(hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, trg, hidden, encoder_out):\n","        trg = trg.unsqueeze(0)\n","        embed = self.dropout(self.embed(trg))\n","        decoder_out, hidden = self.lstm(embed, hidden)\n","        \n","        # we'll go over how these are computed later\n","        atten, context = self.attention(decoder_out, encoder_out)\n","        \n","        # \"We employ a simple concatenation layer to combine the \n","        # information from both vectors:\"\n","        atten_hidden = self.wc(torch.cat((decoder_out, context), dim=2)).tanh()\n","\n","        # \"The attentional vector ~h_t is then fed through the softmax layer\n","        # to produce the predictive distribution:\"\n","        out = self.ws(atten_hidden.squeeze(0))\n","        # softmax will be included in loss function\n","        \n","        return out, hidden, atten"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iP6Bk5OC1m_w"},"source":["We now detail how each model type computes\n","the source-side context vector $c_t$\n","\n","![](https://github.com/teddykoker/blog/blob/master/_notebooks/global_attention.png?raw=1)\n","\n","Figure 2: **Global attentional model** - at each time\n","step $t$, the model infers a variable-length alignment weight vector $a_t$ based on the current target\n","state $h_t$ and all source states $\\overline{h}_s$. A global context vector $c_t$\n","is then computed as the weighted average, according to $a_t$, over all the source state\n","\n","## Global Attention\n","\n","The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector $c_t$. In this model type, a variable-length alignment vector $a_t$, whose size\n","equals the number of time steps on the source side,\n","is derived by comparing the current target hidden\n","state $h_t$ with each source hidden state $\\overline{h}_s$:\n","\n","$$ a_t(s)=\\text{align}(h_t, \\overline{h}_s) $$\n","\n","$$ =\\frac{\\exp(\\text{score}(h_t, \\overline{h}_s))}{\\sum_{s'}\\exp(\\text{score}(h_t, \\overline{h}_{s'}))}$$\n","\n","Here, $\\text{score}$ is referred as a content-based function\n","for which we consider three different alternatives:\n","\n","$$\n","\\text{score}(h_t, \\overline{h}_s)\\!=\\!\\begin{cases}\n","    h_t^\\top \\overline{h}_s & dot\\\\\n","    h_t^\\top W_a \\overline{h}_s & general\\\\\n","    v_a^\\top \\tanh(W_a[h_t;\\overline{h}_s]) & concat\n","\\end{cases}\n","$$\n","\n","\n","Given the alignment vector as weights, the context\n","vector $c_t$ is computed as the weighted average over\n","all the source hidden states."]},{"cell_type":"code","metadata":{"id":"sac2OBRr1m_x"},"source":["class Attention(nn.Module):\n","    \"\"\"\n","    Compute alignment vector and context vector from hidden states\n","    \"\"\"\n","    def __init__(self, hidden_dim, score_fn=\"general\"):\n","        super(Attention, self).__init__()\n","        self.score_fn = score_fn\n","        if score_fn == \"general\":\n","            self.w = nn.Linear(hidden_dim, hidden_dim)\n","        if score_fn == \"concat\":\n","            self.w = nn.Linear(hidden_dim * 2, hidden_dim)\n","            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_dim))\n","            \n","    def score(self, decoder_out, encoder_outs):\n","        if self.score_fn == \"dot\":\n","            return torch.sum(decoder_out * encoder_outs, dim=2)\n","        if self.score_fn == \"general\":\n","            return torch.sum(decoder_out * self.w(encoder_outs), dim=2)\n","        if self.score_fn == \"concat\":\n","            decoder_outs = decoder_out.repeat(encoder_outs.shape[0], 1, 1)\n","            cat = torch.cat((decoder_outs, encoder_outs))\n","            return torch.sum(self.v * self.w(cat), dim=2)\n","            \n","    def forward(self, decoder_out, encoder_outs):\n","        score = self.score(decoder_out, encoder_outs)\n","        a = F.softmax(score, dim=0)\n","            \n","        # \"Given the alignment vector as weights, the context vector \n","        # c_t is computed as the weighted average over all the source \n","        # hidden states:\"\n","        context = torch.bmm(\n","            a.transpose(1, 0).unsqueeze(1),\n","            encoder_outs.transpose(1, 0)\n","        ).transpose(1, 0)\n","        return a, context"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uLI3sfHz1m_x"},"source":["> We implemented an attention model that can perform each of the three $\\text{score}$ functions depending on the parameter `score_fn`. *Note: for brevity, we have skipped over the local attention portion of the paper*. With the `Attention` and `Decoder` modules complete, all we need now is a simple encoder, consisting of some stacked LSTMs and a model that puts all of the modules together:"]},{"cell_type":"code","metadata":{"id":"NFXL9KMU1m_y"},"source":["class Encoder(nn.Module):\n","    \"\"\"\n","    Stacked LSTM encoder\n","    \"\"\"\n","    def __init__(self, input_dim, embed_dim, hidden_dim, \n","                 n_layers, dropout):\n","        super(Encoder, self).__init__()\n","        self.embed = nn.Embedding(input_dim, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout) \n","        \n","    def forward(self, src):\n","        embed = self.dropout(self.embed(src))\n","        out, hidden = self.lstm(embed)\n","        return self.dropout(out), hidden\n","    \n","\n","class Model(nn.Module):\n","    \"\"\"\n","    Sequence to Sequence model with attention\n","    \"\"\"\n","    def __init__(self, encoder, decoder):\n","        super(Model, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","    \n","    def forward(self, src, trg, teacher_force_ratio = 0.5):\n","        outs = torch.zeros(\n","            trg.shape[0], trg.shape[1], self.decoder.output_dim\n","        ).to(src.device)\n","        encoder_out, hidden = self.encoder(src)\n","        \n","        x = trg[0]\n","        for t in range(1, trg.shape[0]):\n","            outs[t], hidden, _ = self.decoder(x, hidden, encoder_out)\n","            x = trg[t] if random.random() < teacher_force_ratio else outs[t].argmax(1)\n","            \n","        return outs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPfbnZLX1m_y"},"source":["> With the model implementation complete, we are ready to train. The remainder of the post will be my own words and figures."]},{"cell_type":"markdown","metadata":{"id":"Kfe4wWFv1m_z"},"source":["## Experiments\n","\n","The original paper trained and evaluated on the [WMT14](http://www.statmt.org/wmt14/translation-task.html) German-English translation task, with a training set of 4.5 million sentence pairs. Since their implementation took \"7 - 10 days to completely train a model,\" we will opt for a much smaller training/evaluation dataset: Multi30K ([Elliott et al., 2016](https://arxiv.org/abs/1605.00459)). This dataset consists of 30 thousand English-German sentence pairs, and we will be able to train the model on a single GPU in less than 31 minutes."]},{"cell_type":"code","metadata":{"id":"d079iicoQTMI"},"source":["import spacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9YYcn_7L_6zJ"},"source":["#!python -m spacy download de_core_news_sm\n","from spacy.lang.de import German\n","spacy_de = German()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","%pip install -q torchtext==0.6\n","from torchtext.datasets import Multi30k"],"metadata":{"id":"-0L2c8HXmBMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Legacy loaders: https://pypi.org/project/torchtext/\n","from torchtext.data import Field, BucketIterator\n","\n","# tokenizers\n","spacy_en = spacy.load('en_core_web_sm')"],"metadata":{"id":"LprUyu1jrf3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOobQBZB1m_z"},"source":["tokenize_de = lambda text: [tok.text for tok in spacy_de.tokenizer(text)]\n","tokenize_en = lambda text: [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","# fields\n","SRC = Field(tokenize=tokenize_de, init_token='<sos>',\n","            eos_token=\"<eos>\", lower=True)\n","TRG = Field(tokenize=tokenize_de, init_token='<sos>',\n","            eos_token=\"<eos>\", lower=True)\n","\n","# data\n","train_data, valid_data, test_data = Multi30k.splits(('.de', '.en'), (SRC, TRG))\n","SRC.build_vocab(train_data, min_freq=2)\n","TRG.build_vocab(train_data, min_freq=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqTaLdkJ1m_z"},"source":["Since we're working with less data, we can make our model smaller as well. The paper uses 4 layers for their LSTM models, each with 1000 cells, and 1000-dimensional embeddings. We will use 2 layers for our LSTM models, each with 512 cells, and 256-dimensional embeddings. We will, however, use the same dropout probability (0.2), batch size (128) and uniform weight initialization ($[-0.1, 0.1]$):"]},{"cell_type":"code","metadata":{"id":"36FqPIjU1m_z"},"source":["input_dim = len(SRC.vocab)\n","output_dim = len(TRG.vocab)\n","embed_dim = 256\n","hidden_dim = 512\n","n_layers = 2\n","dropout = 0.2\n","batch_size = 128\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","def init_weights(model):\n","    for param in model.parameters():\n","        nn.init.uniform_(param.data, -0.1, 0.1)\n","\n","train_loader, valid_loader, test_loader = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=batch_size,\n","    device=device\n",")\n","\n","encoder = Encoder(input_dim, embed_dim, hidden_dim, n_layers, dropout)\n","\n","decoder = Decoder(output_dim, embed_dim, hidden_dim, n_layers, dropout)\n","model = Model(encoder, decoder).to(device)\n","model.apply(init_weights);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ohkd-1qk1m_0"},"source":["### Training\n","\n","We now fit the model to our training set using the Adam optimizer and cross-entropy loss:"]},{"cell_type":"code","metadata":{"id":"vczLAGrl1m_0"},"source":["optimizer = torch.optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])\n","num_epochs = 1\n","\n","def step(model, data, criterion, train=False, optimizer=None):\n","    model.train() if train else model.eval()\n","    total_loss = 0\n","    for batch in tqdm(data, leave=False):\n","        if train: optimizer.zero_grad()\n","        pred = model(batch.src, batch.trg)\n","        loss = criterion(pred.view(-1, pred.size(2)), batch.trg.view(-1))\n","        if train:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(data)\n","\n","best_loss = float('inf')\n","train_loss, valid_loss = np.zeros((2, num_epochs))\n","for e in range(num_epochs):\n","    train_loss[e] = step(model, train_loader, criterion, train=True, optimizer=optimizer)\n","    valid_loss[e] = step(model, valid_loader, criterion)\n","    #print(f\"epoch: {e} train_loss: {train_loss[e]:.2f} valid_loss: {valid_loss[e]:.2f}\")\n","    if valid_loss[e] < best_loss:\n","        torch.save(model.state_dict(), 'model.pt')\n","        best_loss = valid_loss[e]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFIZjXug1m_1"},"source":["plt.figure(figsize=(5, 3), dpi=300)\n","plt.plot(train_loss[:10], label=\"Train\")\n","plt.plot(valid_loss[:10], label=\"Valid.\")\n","plt.legend(); plt.ylabel(\"Cross-Entropy Loss\"); plt.xlabel(\"Epoch\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zoBODXw91m_1"},"source":["### Evaluation"]},{"cell_type":"markdown","metadata":{"id":"-rBw07PW1m_1"},"source":["Now that we have a trained model, we can test it on our test set. In addition to cross-entropy loss, another good metric we can use is [perplexity](https://en.wikipedia.org/wiki/Perplexity), which is a measurement of the models ability to predict the target translation. This can be calculated as simply $\\exp(H(p))$, where $H(p)$ is the cross-entropy loss of the model over the test set. The lower the perplexity, the better the model is at predicting the target translation."]},{"cell_type":"code","metadata":{"id":"rnjddLni1m_2"},"source":["model.load_state_dict(torch.load(\"model.pt\"))\n","test_loss = step(model, test_loader, criterion)\n","print(f\"Test loss: {test_loss:.2f}\")\n","print(f\"Test perplexity: {np.exp(test_loss):.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2GIOzLw1m_2"},"source":["Our model achieves a perplexity of 13.84. This is worse then the paper's best perplexity of 5.9, but there is no way to directly compare as we are using a different dataset. There are a few ways we could improve the model to better match the paper's results:\n","\n","1. Use bidirectional LSTM models; The model should be able to better \"understand\" the context of each word by looking at words both before and after each word.\n","\n","2. More training data; The model could likely benefit from more training data, such as the aforementioned WMT14 dataset.\n","\n","3. Larger model and ensemble; The paper used a much larger model, and a technique known as [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning). By combining multiple model's outputs into one, we could obtain better predictive performance.\n","\n","With that out of the way, lets run our model on some test sentences!"]},{"cell_type":"code","metadata":{"id":"JadMaUgb1m_2"},"source":["@torch.no_grad()\n","def translate(sentence, model, device, max_len=50):\n","    model.eval()\n","    src = SRC.process([sentence]).to(device)\n","    trg = torch.ones(1, dtype=torch.int64).to(device) * TRG.vocab.stoi[TRG.init_token]\n","    trgs, attention = [], []\n","    encoder_out, hidden = model.encoder(src)\n","    \n","    for t in range(max_len):\n","        trg, hidden, atten = model.decoder(trg, hidden, encoder_out)\n","        trg = trg.argmax(1)\n","        trgs.append(trg)\n","        attention.append(atten.T)\n","        if trg == TRG.vocab.stoi[TRG.eos_token]: break\n","\n","    trg = [TRG.vocab.itos[i] for i in trgs]\n","    src = [SRC.vocab.itos[i] for i in src]\n","    attention = torch.cat(attention).cpu().numpy()[:-1, 1:]\n","    return src, trg, attention"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YSZFPYZG1m_2"},"source":["Let's take a look at our first example:"]},{"cell_type":"code","metadata":{"id":"kOvv0f-I1m_2"},"source":["example = 6\n","src, trg = test_data[example].src, test_data[example].trg\n","print(f\"Source: {' '.join(src)}\")\n","print(f\"Target: {' '.join(trg)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iqxk8RZm1m_3"},"source":["And now our predicted translation:"]},{"cell_type":"code","metadata":{"id":"fMKNjxTu1m_3"},"source":["src, pred, attention = translate(src, model, device)\n","print(f\"Prediction: {' '.join(pred[:-1])}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNhnfCnr1m_3"},"source":["Not bad; let's try another:"]},{"cell_type":"code","metadata":{"id":"tHPaeAFQ1m_3"},"source":["example = 1\n","src, trg = test_data[example].src, test_data[example].trg\n","print(f\"Source: {' '.join(src)}\")\n","print(f\"Target: {' '.join(trg)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WeV5cw8ZlCv"},"source":["sentenceToTranslate = \"ein boston terrier läuft über saftig-grünes gras vor einem weißen zaun.\"\n","sentenceTokenized   = tokenize_de(sentenceToTranslate)\n","print(sentenceTokenized)\n","\n","src, pred, attention = translate(sentenceTokenized, model, device)\n","print(f\"Prediction: {' '.join(pred[:-1])}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDG6sHyy1m_3"},"source":["The model did not have the word *saftig-grünes* 'lush green' in it's vocabulary, so it was unable to translate that word. Although the sentences are not perfect translations, they are fairly coherent, and certainly better than a human could do with half an hour of learning German. One great thing about the model is that we can view the alignment weights $a_t$, and see which source word the model is \"concentrating\" on for each output word."]},{"cell_type":"code","metadata":{"id":"FShw_n5i1m_4"},"source":["def plot_attention(src, trg, attention):\n","    fig = plt.figure(figsize=(4, 4), dpi=300)\n","    ax = fig.add_subplot(111)\n","    ax.matshow(attention)\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.set_xticklabels([''] + src, rotation=60)\n","    ax.set_yticklabels([''] + trg) \n","    \n","plot_attention(src, pred, attention)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dtWz_BQj1m_4"},"source":["We can see that model aligns each German source word with one of the predicted words. Terrier with dog, läuft with runs, ... etc. \n","\n","## Conclusion\n","\n","Attention is an important concept to know when it comes to understanding the current state-of-the-art language models. Now that we've created a simple implementation for machine translation, we'll be able to expand upon this model for different tasks. Stay tuned for my next post, where we'll create a small version of Google's BERT ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)) from scratch. Be sure to follow me on [twitter](https://twitter.com/teddykoker) for updates! This notebook can be found on [Github](https://github.com/teddykoker/blog/blob/master/_notebooks/2020-02-25-nlp-from-scratch-annotated-attention.ipynb), or be run on [Google Colab](https://colab.research.google.com/github/teddykoker/blog/blob/master/_notebooks/2020-02-25-nlp-from-scratch-annotated-attention.ipynb)"]}]}