{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9TqNz3HLbr2e"},"source":["# Examples for Homework 6.1: BERT masked words in English and Mandarin Chinese\n","Dartmouth College, LING48, Spring 2023<br>\n","Rolando Coto-Solano (Rolando.A.Coto.Solano@dartmouth.edu)\n","\n","I did not write this code. This code is from Stack Overflow:<br>\n","https://stackoverflow.com/questions/54978443/predicting-missing-words-in-a-sentence-natural-language-processing-model\n","\n","This file uses a pre-trained BERT to predict the word missing from a sentence. In this file, we are using an example in English. Study the code, and then carry out the following tasks:\n","\n","(1)\tStudy the links below, so you can gain a better understanding of how BERT works:\n","\n","http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/<br>\n","http://jalammar.github.io/illustrated-bert/\n","\n","(2)\tChange the model from an English-language BERT to the Mandarin Chinese pretrained BERT. You can find the names of the models here:<br>https://huggingface.co/transformers/pretrained_models.html\n","\n","(3)\tChange the input so that it predicts a missing Chinese character. For example, for the sentence 你是中 __ 人吗  *nǐ shì zhōng ___ rén ma* , it should predict that the missing character is 国 guó (which would form the sentence “are you Chinese?”). \n","\n","(4)\tSubmit a PDF/LibreOffice/Word document with a brief explanation of the lines that you changed in the code. Also, please submit the Python code with your modification, and a screenshot of your results.\n"]},{"cell_type":"code","metadata":{"id":"6EIsh1Svwx5v"},"source":["!pip install pytorch-pretrained-bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4hOERZ-wQMA"},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMk75Yhyw2i1"},"source":["# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qk_TOvffw5PE"},"source":["# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","\n","text = '[CLS] I want to [MASK] a car because they are cheap . [SEP]'\n","tokenized_text = tokenizer.tokenize(text)\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","masked_index = tokenized_text.index('[MASK]')\n","\n","# Create the segments tensors.\n","segments_ids = [0] * len(tokenized_text)\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n","\n","# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-cased')\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNED-D72xEcE"},"source":["# Predict all tokens\n","with torch.no_grad():\n","    predictions = model(tokens_tensor, segments_tensors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTA9IrBDxILc"},"source":["predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","\n","print(predicted_token)"],"execution_count":null,"outputs":[]}]}