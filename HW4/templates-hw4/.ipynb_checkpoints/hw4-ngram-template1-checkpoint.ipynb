{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXcge0wSzBb6"
   },
   "source": [
    "# Exercise 4.1A: Text Generation Using N-Grams\n",
    "Kevin King (Kevin.M.King.24@dartmouth.edu)<br>\n",
    "Dartmouth College, LING48, Spring 2023\n",
    "\n",
    "Please study the links below and attempt to modify the program according to the homework instructions.\n",
    "\n",
    "Documentation of the NLTK.LM package:<br>\n",
    "https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "How to extract n-gram probabilities:<br>\n",
    "https://stackoverflow.com/questions/54962539/how-to-get-the-probability-of-bigrams-in-a-text-of-sentences\n",
    "\n",
    "My Implementation (`generate_text` function below): \n",
    "* In the code below, we start by downloading the necessary packages and resources for the analysis\n",
    "* Preprocessed the tokenized text for language modelling\n",
    "* Trained an n-gram maximum likelihood estimation model\n",
    "* Used `model.generate()` to get a 100-word sequence out of an n-gram\n",
    "* Got the counts of a selected unigram, bigram, and trigram using the `model.counts()` function\n",
    "* Got the probabilities of a selected unigram, bigram, and trigram using the `model.score()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MXbg5lhzpAaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.4)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk==3.4) (1.16.0)\n",
      "Requirement already satisfied: singledispatch in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk==3.4) (4.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upgrade from version in the VM\n",
    "!pip install -U nltk==3.4\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N1vCMVOyo8AA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io \n",
    "import random\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from nltk.lm import MLE, NgramCounter, Vocabulary\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, sent_tokenize, bigrams, trigrams\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SLyEbEgUq-RW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1pW_rn9oZi3Ax23-ezP8K3AcR5QhtLR-a\n",
      "To: /Users/kevin/Desktop/Dartmouth/2022-23/23S/CS72/HW4/templates-hw4/ling28-corpora.tar.gz\n",
      "100%|██████████████████████████████████████| 22.8M/22.8M [00:01<00:00, 16.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download and decompress corpora\n",
    "url = \"https://drive.google.com/uc?id=1pW_rn9oZi3Ax23-ezP8K3AcR5QhtLR-a\"\n",
    "output = 'ling28-corpora.tar.gz'\n",
    "gdown.download(url, output, quiet=False)\n",
    "!tar -xf ling28-corpora.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dWECRfnWp02P"
   },
   "outputs": [],
   "source": [
    "# Open file\n",
    "file = io.open('english - shakespeare.txt', encoding='utf8')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA-utVqO2goN"
   },
   "source": [
    "#### Preprocess the tokenized text for language modelling\n",
    "https://stackoverflow.com/questions/54959340/nltk-language-modeling-confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates 100-word sequences out of an n-gram\n",
    "def generate_text(ngrams):\n",
    "    # Preprocess the tokenized text for language modelling\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), ngrams))]\n",
    "    train, vocab = padded_everygram_pipeline(ngrams, paddedLine)\n",
    "\n",
    "    # Train an n-gram maximum likelihood estimation model.\n",
    "    model = MLE(n) \n",
    "    model.fit(train, vocab)\n",
    "    \n",
    "    # Add word tokens into array \n",
    "    generated_text = model.generate(100)\n",
    "    \n",
    "    return model, generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqfeSN9X2eOl"
   },
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Sequence: \n",
      "by to are vnder , 'd all for for of montague to i i too precisely so ; my in , i rosencrantz severally one bolingbroke are mrs. april disturbed o , device own off than ! am made it in king silken farewell child comes more ' a , for will it nature of plummet to what that , you and richmond , ? . my ; will nobleman men , [ . rich from and all with that it . the his will better nose not this all do him these my not thee thine . hast be\n"
     ]
    }
   ],
   "source": [
    "model1, text1 = generate_text(1)\n",
    "print(\"Unigram Sequence: \\n\" + ' '.join(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Sequence: \n",
      "slender accident , and not ; which wear hair at 'em be a liberal rewarder of saucy friar lodowick , they shall we see the devil ! we go home to leave you be not ours of that is a wicked , and such reasons . ] saturninus . no such inevitable prosecution of flesh—you have employ a bargain . king of heaven , i pleas 'd against the wit going back , and life-preserving rest ? soothsayer . perge , which rather give me , to be not . he too far as levels with your pardon me so\n"
     ]
    }
   ],
   "source": [
    "model2, text2 = generate_text(2)\n",
    "print(\"Bigram Sequence: \\n\" + ' '.join(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Sequence: \n",
      "not ; they have privilege to live , shall we dine . this business soundly . duke . my masters , for here comes the better at proverbs by how much i am very well , bully doctor ! shallow . i go ; i can not fight ; the duke he shall feel , to smile again ; for whose sake did i ne'er endured . cerimon . madam , the king , unto the worms were hallow 'd that ; and easy it is not thy kindness last longer telling than thy master here i am not gamesome\n"
     ]
    }
   ],
   "source": [
    "model3, text3 = generate_text(3)\n",
    "print(\"Trigram Sequence: \\n\" + ' '.join(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four-gram Sequence: \n",
      "doth run his course . if heaven do dwell . exeunt clown , who commands them , for a search , seek , but now it is not . portia . is there , diomed . call him hither . re-enter troilus . what , out of his thoughts , wherein i see on thee , prithee , pretty youth , and courtezan say now , sir , stands in record , and , that all , that comes a-wooing , _priami_ , is done ; and let poor volke pass . [ within ] who 's here ! let\n"
     ]
    }
   ],
   "source": [
    "model4, text4 = generate_text(4)\n",
    "print(\"Four-gram Sequence: \\n\" + ' '.join(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Counts ==\n",
      "Unigram Count ('my'): 12618\n",
      "Bigram Count ('my good'): 228\n",
      "Trigram Count ('my good lord'): 77\n"
     ]
    }
   ],
   "source": [
    "print(\"== Counts ==\")\n",
    "\n",
    "unigram_count = model1.counts['my']\n",
    "print(\"Unigram Count ('my'): \" + str(unigram_count))\n",
    "\n",
    "bigram_count = model2.counts[['my']]['good']\n",
    "print(\"Bigram Count ('my good'): \" + str(bigram_count))\n",
    "\n",
    "trigram_count = model3.counts[['good', 'my']]['lord']\n",
    "print(\"Trigram Count ('my good lord'): \" + str(trigram_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilities (using the trigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Probabilities ==\n",
      "Unigram Prob ('my'): 0.010839898868846502\n",
      "Bigram Prob ('my good'): 0.01806942463147884\n",
      "Trigram Prob ('my good lord'): 0.5614035087719298\n"
     ]
    }
   ],
   "source": [
    "print(\"== Probabilities ==\")\n",
    "\n",
    "unigram_score = model3.score('my')\n",
    "print(\"Unigram Prob ('my'): \" + str(unigram_score))\n",
    "\n",
    "bigram_score = model3.score('good', 'my'.split())\n",
    "print(\"Bigram Prob ('my good'): \" + str(bigram_score))\n",
    "\n",
    "trigram_score = model3.score('lord', 'my good'.split())\n",
    "print(\"Trigram Prob ('my good lord'): \" + str(trigram_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4-ngram-template1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
